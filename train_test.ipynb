{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering, Blip2Processor, Blip2Model, Blip2ForConditionalGeneration\n",
    "\n",
    "from word2number import w2n\n",
    "import ot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Blip2 for image captioning(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\", load_in_8bit=True, device_map={\"\": 0}, torch_dtype=torch.float16\n",
    ") \n",
    "\n",
    "\n",
    "image_folder = \"data/shape1/\"\n",
    "output_file = \"data/shape1.txt\"\n",
    "\n",
    "\n",
    "image_files = [img for img in os.listdir(image_folder) if img.endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "image_files.sort(key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for image_name in image_files:\n",
    "        image_path = os.path.join(image_folder, image_name)\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "     \n",
    "        inputs = processor(images=image, return_tensors=\"pt\", padding=True).to(device, torch.float16)\n",
    "\n",
    "        generated_ids = model.generate(**inputs)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip() \n",
    "    \n",
    "        f.write(f\"{image_name}: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsistencyClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ConsistencyClassifier, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out_proj = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def sinkhorn_distance(x, y, epsilon=1.0, num_iters=20):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Tensor of shape (batch_size, num_features_x, feature_dim)\n",
    "        y (torch.Tensor): Tensor of shape (batch_size, num_features_y, feature_dim)\n",
    "        epsilon (float): \n",
    "        num_iters (int): Sinkhorn \n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor:\n",
    "    \"\"\"\n",
    "    x_col = x.unsqueeze(2)  # (batch_size, num_features_x, 1, feature_dim)\n",
    "    y_lin = y.unsqueeze(1)  # (batch_size, 1, num_features_y, feature_dim)\n",
    "    C = torch.sum((x_col - y_lin) ** 2, dim=-1)  # (batch_size, num_features_x, num_features_y) 32 9 144\n",
    "    \n",
    "    C = C / C.max()\n",
    "    #  e^(-C/epsilon) 32 9 144\n",
    "    K = torch.exp(-C / epsilon)\n",
    "    log_K = -C / epsilon\n",
    "\n",
    "    # a, b means\n",
    "    a = torch.ones(x.size(0), x.size(1), device=x.device) / x.size(1)\n",
    "    b = torch.ones(y.size(0), y.size(1), device=y.device) / y.size(1)\n",
    "\n",
    "    # Sinkhorn \n",
    "    log_u = torch.zeros_like(a)\n",
    "    log_v = torch.zeros_like(b)\n",
    "    for _ in range(num_iters):\n",
    "        log_u = torch.log(a) - torch.logsumexp(log_K + log_v.unsqueeze(1), dim=2)\n",
    "        log_v = torch.log(b) - torch.logsumexp(log_K.transpose(-2, -1) + log_u.unsqueeze(1), dim=2)\n",
    "\n",
    "\n",
    "    log_T = log_u.unsqueeze(-1) + log_K + log_v.unsqueeze(-2)\n",
    "    T = torch.exp(log_T) \n",
    "\n",
    "\n",
    "    distance = torch.sum(T * C, dim=(-2, -1))\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "def batch_loader(file_path, batch_size=32):\n",
    "    batch_images = []\n",
    "    batch_questions = []\n",
    "    batch_labels = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(',')\n",
    "            if len(parts) == 4:\n",
    "                question = parts[1]\n",
    "                answer = parts[2]\n",
    "                image_path = parts[3]\n",
    "\n",
    "                try:\n",
    "                    img = Image.open(image_path).resize((384, 384))\n",
    "                    batch_images.append(img)\n",
    "                    batch_questions.append(question)\n",
    "                    ans1 = w2n.word_to_num(answer)\n",
    "                    batch_labels.append(str(ans1))\n",
    "\n",
    "                    if len(batch_images) == batch_size:\n",
    "                        yield batch_images, batch_questions, batch_labels\n",
    "                        batch_images, batch_questions, batch_labels = [], [], []\n",
    "                except IOError:\n",
    "                    print(f\"Error opening image {image_path}\")\n",
    "        if batch_images:  # Yield any remaining data as the last batch\n",
    "            yield batch_images, batch_questions, batch_labels\n",
    "\n",
    "\n",
    "def process_and_predict(images, questions, processor, model):\n",
    "\n",
    "    encoding = processor(images, questions, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**encoding)\n",
    "    \n",
    "def valid(model, best_acc, processor):\n",
    "    print(\"--------------------this is validation------------------------\")\n",
    "    accuracy = []\n",
    "    model.cpu()\n",
    "    model.eval()\n",
    "    for images, questions, labels in batch_loader('data/val_data.txt'):#ata/val_data\n",
    "        outputs = process_and_predict(images, questions, processor, model)\n",
    "        print(outputs.logits)\n",
    "        probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "        max_prob_indices = torch.argmax(probabilities, dim=1)\n",
    "        word_list = []\n",
    "        for i,k in  enumerate(max_prob_indices):\n",
    "            print(questions[i])\n",
    "            print(\"-------------ground truth --------------\")\n",
    "            print(labels[i])\n",
    "            print(\"-------------our answer --------------\")\n",
    "            print(model.config.id2label[int(k)])\n",
    "            word_list.append(model.config.id2label[int(k)])\n",
    "        matches = sum(1 for x, y in zip(word_list, labels) if x == y)\n",
    "        probability = matches / len(labels)\n",
    "        print(\"-------------batch accuracy --------------\")\n",
    "        accuracy.append(probability)\n",
    "        print(probability)\n",
    "\n",
    "    average = sum(accuracy) / len(accuracy)\n",
    "    print(\"-------------average accuracy --------------\")\n",
    "    print(average)\n",
    "    if average> best_acc:\n",
    "        best_acc = average \n",
    "        model.save_pretrained('checkpoint/')\n",
    "        print(\"save in:{save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main train loop(val)\n",
    "\n",
    "for demo purpose, we don't show the whole training process on our own laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViltProcessor.from_pretrained(\"./checkpoint\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"./checkpoint\").cuda()\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "criteria = nn.BCELoss()\n",
    "cls = ConsistencyClassifier(hidden_size=768)\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(cls.parameters()) + list(model.parameters()), lr=0.0001\n",
    ")\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=15, eta_min=0)\n",
    "\n",
    "beta = 0.001\n",
    "theta = 0.000005\n",
    "\n",
    "\n",
    "for epoch in range(15):\n",
    "    model.cuda()\n",
    "    cls.cuda()\n",
    "    cls.train()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    avg_train_loss = []\n",
    "    avg_train_acc = []\n",
    "    num = 0\n",
    "\n",
    "    for images, questions, labels in batch_loader(\n",
    "        \"data/train_data.txt\", batch_size=32\n",
    "    ):\n",
    "\n",
    "        # image_list = [transforms.ToTensor()(image) for image in images]\n",
    "        label_list = [model.config.label2id[label] for label in labels]\n",
    "        labels_tensor = torch.tensor(label_list).cuda()\n",
    "\n",
    "        encoding = processor(\n",
    "            images, questions, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "        for feature, data in encoding.data.items():\n",
    "            encoding.data[feature] = data.cuda()\n",
    "\n",
    "        # outputs = model(**encoding)\n",
    "\n",
    "        outputs = model.vilt(**encoding)\n",
    "        text_feature = outputs.last_hidden_state[:, 1:10, :]\n",
    "        image_feature = outputs.last_hidden_state[:, -144:, :]\n",
    "\n",
    "        # SinkhornDistance\n",
    "        normalized_text_features = text_feature / text_feature.sum(dim=1, keepdim=True)\n",
    "        normalized_image_features = image_feature / image_feature.sum(\n",
    "            dim=1, keepdim=True\n",
    "        )\n",
    "\n",
    "        #\n",
    "        OT_distance = sinkhorn_distance(\n",
    "            normalized_text_features, normalized_image_features\n",
    "        )\n",
    "        loss2 = OT_distance.mean()\n",
    "\n",
    "        logits = model.classifier(outputs.pooler_output)\n",
    "        labels = torch.FloatTensor([[1] for i in range(len(labels))])\n",
    "        loss1 = criteria(cls(outputs.pooler_output), labels.cuda())\n",
    "\n",
    "        # logits = outputs.logits\n",
    "        optimizer.zero_grad()\n",
    "        loss = (\n",
    "            loss_function(logits, labels_tensor)\n",
    "            + beta * loss1\n",
    "            + theta * loss2\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        num += len(labels)\n",
    "\n",
    "        train_loss += loss.item() * len(labels)\n",
    "        ret, predictions = torch.max(logits.data, 1)\n",
    "        correct_counts = predictions.eq(labels_tensor.data.view_as(predictions))\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "        train_acc += acc.item() * len(labels)\n",
    "\n",
    "        avg_train_loss.append(train_loss)\n",
    "        avg_train_acc.append(train_acc)\n",
    "\n",
    "    avg_train_loss1 = sum(avg_train_loss) / num\n",
    "    avg_train_acc1 = sum(avg_train_acc) / num\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}, Average Training Loss: {avg_train_loss1}, Average Training Accuracy: {avg_train_acc1}\"\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "    best_acc = 0\n",
    "    valid(model, best_acc, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "\n",
    "for images, questions, labels in batch_loader('data/test_data.txt'):\n",
    "    outputs = process_and_predict(images, questions, processor, model)\n",
    "    print(outputs.logits)\n",
    "\n",
    "    probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "    max_prob_indices = torch.argmax(probabilities, dim=1)\n",
    "    word_list = []\n",
    "\n",
    "    for i,k in  enumerate(max_prob_indices):\n",
    "        print(questions[i])\n",
    "        print(\"-------------ground truth --------------\")\n",
    "        print(labels[i])\n",
    "        print(\"-------------our answer --------------\")\n",
    "        print(model.config.id2label[int(k)])\n",
    "        word_list.append(model.config.id2label[int(k)])\n",
    "\n",
    "    matches = sum(1 for x, y in zip(word_list, labels) if x == y)\n",
    "    probability = matches / len(labels)\n",
    "    print(\"-------------batch accuracy --------------\")\n",
    "    accuracy.append(probability)\n",
    "    print(probability)\n",
    "\n",
    "average = sum(accuracy) / len(accuracy)\n",
    "print(\"-------------average accuracy --------------\")\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ViltProcessor.from_pretrained(\"./checkpoint\")\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"./checkpoint\").cuda()\n",
    "\n",
    "\n",
    "def predict(image_path, question, processor, model):\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    image = Image.open(image_path).resize((384, 384)) \n",
    "\n",
    "    encoding = processor(image, question, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Move the encoded tensors to the same device as the model\n",
    "    for key in encoding:\n",
    "        encoding[key] = encoding[key].to(device)\n",
    "\n",
    "    # Move the model to the device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Generate outputs using the model\n",
    "    output = model(**encoding)\n",
    "\n",
    "    prob = torch.softmax(output.logits, dim=1)\n",
    "    max_prob_indice = torch.argmax(prob, dim=1)\n",
    "    word_list = []\n",
    "\n",
    "    print(question)\n",
    "\n",
    "    for i,k in  enumerate(max_prob_indice):\n",
    "\n",
    "        print(\"-------------our answer --------------\")\n",
    "        print(model.config.id2label[int(k)])\n",
    "        word_list.append(model.config.id2label[int(k)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"data/shape/10.png\"\n",
    "question = \"how many green squares are in the image?\"\n",
    "\n",
    "predict(image_path,question,processor,model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
